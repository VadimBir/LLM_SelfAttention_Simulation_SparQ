{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T14:35:00.282876Z",
     "start_time": "2024-07-03T14:34:54.761742Z"
    }
   },
   "source": [
    "# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import llminference as L\n",
    "import llminference.experiments as xp\n",
    "import torch\n",
    "torch.set_num_threads(32)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High-level Experiment API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T14:35:00.510748Z",
     "start_time": "2024-07-03T14:35:00.296995Z"
    }
   },
   "source": [
    "out = xp.run_one(xp.Experiment(\n",
    "    \"test\",\n",
    "    task=xp.Task(\"squad\", shots=1, samples=10, confusion_contexts=0),\n",
    "    model=\"EleutherAI/pythia-410m\",\n",
    "    execution=xp.Execution(device=\"cpu\", dtype=\"float32\", batch_size=10, pipeline_stages=1, wandb=False),\n",
    "    sparsity=xp.Sparsity(\"ann\", k=64, local_k=16, score=\"sparse_q\", rank=16, reallocate_to_mean_value=True),\n",
    "))\n",
    "display({k: v for k, v in out.items() if k not in {\"model_config\", \"results\"}})"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual evaluation\n",
    "\n",
    "This codebase provides & interfaces with multiple harnesses for evaluating language models, with a particular focus on text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "adapter = L.Adapter.from_pretrained(\"EleutherAI/pythia-410m\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQuAD\n",
    "\n",
    "We evaluate SQuAD using a custom harness. It is quite bare-bones, so it's easy to get hands-on with the data & results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "from llminference.tasks import qa\n",
    "\n",
    "squad_data = qa.SQuAD.data()\n",
    "examples = [qa.add_few_shot_prompt(squad_data[i], k=1, prompt_template=qa.get_default_prompt_template(adapter.model.config._name_or_path, shots=1))\n",
    "            for i in range(10)]\n",
    "display(examples[3])\n",
    "results = list(qa.evaluate(adapter, examples, batch_size=10))\n",
    "display(results[3])\n",
    "print(\"accuracy\", sum(r[\"match\"] for r in results) / len(results))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outcompare\n",
    "\n",
    "Outcompare is a custom harness for comparing the greedy generations of a language model against a reference output (e.g. the same model, before quantisation to low-precision).\n",
    "\n",
    "Note - this requires data from `generate_outcompre_datasets.py`, or the `data` branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "outcompare_data = L.tasks.outcompare.Dataset.load(\"../data/pythia-410m.json\")\n",
    "display(L.tasks.outcompare.evaluate(adapter.model, outcompare_data, batch_size=16, limit=64))"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, deliberately mess up the model & see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "adapter.model.gpt_neox.layers[4].attention.dense.weight.data.fill_(0)\n",
    "display(L.tasks.outcompare.evaluate(adapter.model, outcompare_data, batch_size=16, limit=64))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print()"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
