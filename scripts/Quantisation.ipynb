{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n",
    "\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "import llminference.methods.quantisation as Q"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantising whole models\n",
    "\n",
    "Torch models can be quantised using `Q.quantise_model()`, which takes a list of `(pattern, format)` pairs. The pattern matches against the name from `module.named_parameters()`, and the matching format is used to quantise each tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "model = transformers.AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-160m\")\n",
    "base_model_bytes = 2 * sum(p.nelement() for p in model.parameters())\n",
    "\n",
    "def error(qmodel: transformers.PreTrainedModel) -> float:\n",
    "    return torch.mean(torch.stack([\n",
    "        ((p - q) ** 2).mean().detach()\n",
    "        for p, q in zip(model.parameters(), qmodel.parameters())\n",
    "    ]))\n",
    "\n",
    "spec =  [(\".*\", Q.FP16)]\n",
    "qmodel = copy.deepcopy(model)\n",
    "qbytes = Q.quantise_model(qmodel, spec)\n",
    "print(f\"Compressed size: {qbytes / base_model_bytes:.0%}\")\n",
    "print(f\"Error: {error(qmodel):.2e}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "spec = [\n",
    "    (\"embed_out\", Q.channel_scaling_format(Q.parse(\"E2M5\"), per=\"output\")),\n",
    "    (\".*\", Q.group_scaling_format(Q.parse(\"E0M3\"), grouping=\"input\", group_size=8, scale_format=Q.parse(\"E5M2\"))),\n",
    "]\n",
    "qmodel = copy.deepcopy(model)\n",
    "qbytes = Q.quantise_model(qmodel, spec)\n",
    "print(f\"Compressed size: {qbytes / base_model_bytes:.0%}\")\n",
    "print(f\"Error: {error(qmodel):.2e}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "spec = [\n",
    "    (\"embed_out\", Q.channel_scaling_format(Q.parse(\"E2M5\"), per=\"output\")),\n",
    "    (\".*\", Q.group_scaling_format(Q.parse(\"E0M3\"), grouping=\"input\", group_size=8,\n",
    "                                  scale_format=Q.tensor_scaling_format(Q.ExpCeilFormat(4, 0), scale_format=Q.ExpCeilFormat(8, 0)))),\n",
    "]\n",
    "qmodel = copy.deepcopy(model)\n",
    "qbytes = Q.quantise_model(qmodel, spec)\n",
    "print(f\"Compressed size: {qbytes / base_model_bytes:.0%}\")\n",
    "print(f\"Error: {error(qmodel):.2e}\")"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual parameters\n",
    "\n",
    "Parameter tensors can be quantised using `Q.LinearScalingFormat`, which picks per-(tensor, channel, group) scaling factors, based on `tensor.abs().max()`.\n",
    "\n",
    "This provides the main functionality `quantise()` and `count_bits()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "tensor = model.gpt_neox.layers[3].attention.query_key_value.weight.data\n",
    "\n",
    "qtensor_tensor = Q.tensor_scaling_format(Q.parse(\"E0M3\")).quantise(tensor)\n",
    "qtensor_input = Q.channel_scaling_format(Q.parse(\"E0M3\"), per=\"input\").quantise(tensor)\n",
    "qtensor_output = Q.channel_scaling_format(Q.parse(\"E0M3\"), per=\"output\").quantise(tensor)\n",
    "\n",
    "group_format = Q.group_scaling_format(Q.parse(\"E0M3\"), grouping=\"input\", group_size=8)\n",
    "qtensor_group = group_format.quantise(tensor)\n",
    "print(f\"           FP16 size: {Q.FP16.count_bits(tensor.shape) / 8 / 2**20:.2f} MB\")\n",
    "print(f\"Group-quantised size: {group_format.count_bits(tensor.shape) / 8 / 2**20:.2f} MB\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "size = 128\n",
    "norm = matplotlib.colors.PowerNorm(0.5, vmin=0, vmax=tensor.abs().max())\n",
    "\n",
    "_, axs = plt.subplots(1, 5, figsize=(4*4, 4))\n",
    "axs[0].imshow(tensor[:size, :size].abs(), cmap=\"gray\", norm=norm)\n",
    "axs[0].set_title(\"(Original)\")\n",
    "\n",
    "for ax, spec, qtensor in zip(axs[1:], [\"tensor\", \"input\", \"output\", \"group(8)\"], [qtensor_tensor, qtensor_input, qtensor_output, qtensor_group]):\n",
    "    ax.imshow((tensor - qtensor)[:size, :size].abs(), cmap=\"hot\", norm=norm)\n",
    "    ax.set_title(f\"Per-{spec} scales\")\n",
    "\n",
    "plt.suptitle(\"Error (E0M3 vs original)\", y=0.9)\n",
    "for ax in axs:\n",
    "    ax.set_xticks([]) ; ax.set_yticks([])\n",
    "    ax.set_xlabel(\"input\") ; ax.set_ylabel(\"output\")"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scalar formats\n",
    "\n",
    "Scalar formats subclass `Q.Format`, providing `max_absolute_value` as well as `quantise()` and `count_bits()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "e2m1 = Q.parse(\"E2M1\")\n",
    "print(f\"{e2m1} max value {e2m1.max_absolute_value}\")\n",
    "\n",
    "xs = torch.tensor([-1.8, 0, 5])\n",
    "print(f\"{xs} --> {e2m1} quantise --> {e2m1.quantise(xs)}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "xs = torch.linspace(-12, 12, 1000)\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.plot(xs, xs, \"k\")\n",
    "for fmt in [Q.parse(\"E0M3\"),Q.parse(\"E2M1\"),Q.parse(\"E3M0\")]:\n",
    "    plt.plot(xs, fmt.quantise(xs), label=str(fmt))\n",
    "plt.legend()\n",
    "plt.xlabel(\"x\"); plt.ylabel(\"quantise(x)\");"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
